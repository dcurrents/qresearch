{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictive Analytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load_ext watermark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Durga 2017-01-11 \n",
      "\n",
      "CPython 2.7.12\n",
      "IPython 5.1.0\n",
      "\n",
      "numpy 1.11.1\n",
      "pandas 0.18.1\n",
      "matplotlib 1.5.3\n",
      "sklearn 0.18.1\n",
      "keras 1.2.0\n",
      "seaborn 0.7.1\n"
     ]
    }
   ],
   "source": [
    "%watermark -a 'Durga' -v -d -p numpy,pandas,matplotlib,sklearn,keras,seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# CSS Fileb\n",
    "from IPython.core.display import HTML\n",
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# general libs\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "\n",
    "#plotting libs\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.dates import DayLocator, HourLocator, MinuteLocator, AutoDateLocator, DateFormatter\n",
    "\n",
    "# date time libs\n",
    "import datetime as dt\n",
    "from datetime import timedelta\n",
    "import statsmodels.api as sm \n",
    "\n",
    "# ml libs\n",
    "import math\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Style Setting\n",
    "sns.set(style=\"ticks\", color_codes=True)\n",
    "sns.set_context(\"notebook\")\n",
    "sns.set_style(\"darkgrid\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Notebook Style Setting\n",
    "css = open('styles/style-table.css').read() + open('styles/style-notebook.css').read()\n",
    "HTML('<style>{}</style>'.format(css));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>\n",
       "code_show=true; \n",
       "function code_toggle() {\n",
       " if (code_show){\n",
       " $('div.input').hide();\n",
       " } else {\n",
       " $('div.input').show();\n",
       " }\n",
       " code_show = !code_show\n",
       "} \n",
       "$( document ).ready(code_toggle);\n",
       "</script>\n",
       "<form action=\"javascript:code_toggle()\"><input type=\"submit\" value=\"Toggle Code On/Off\"></form>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HTML('''<script>\n",
    "code_show=true; \n",
    "function code_toggle() {\n",
    " if (code_show){\n",
    " $('div.input').hide();\n",
    " } else {\n",
    " $('div.input').show();\n",
    " }\n",
    " code_show = !code_show\n",
    "} \n",
    "$( document ).ready(code_toggle);\n",
    "</script>\n",
    "<form action=\"javascript:code_toggle()\"><input type=\"submit\" value=\"Toggle Code On/Off\"></form>''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Say you are an admin with 100+ volumes and want to know approximately when a given volume capacity will be full? Or approximately what would be the available capacity of the given volume in 4 weeks? Or you might want this information at environment level i.e., say how many volumes are at risk of filling up in next 4 weeks and how much is the risk etc. Or may be your interest is not necessarily capacity availability but some other resource availability.\n",
    "\n",
    "There could be many reasons for needing this info. Ex: To plan & proactively do the necessary procurements, rebalancing etc.  \n",
    "\n",
    "This notebook utilizes Deep Learning (LSTM RNN Neural network) & Montecarlo Simulation techniques to do the above. Some key challenges here are (a) different volumes have different usage behaviors and (b) different volumes have different trend & seasonalities in their data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### APIs..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##### DATA API ###########\n",
    "\n",
    "def read_metric_data(csv_name):\n",
    "    '''\n",
    "        Read data from a given csv file.\n",
    "        CSV format: [server_name, timestamp, value]\n",
    "        Timestamp format: YYYY-MM-DD HH:mm:SS\n",
    "        Lines begining with # are ignored as comments.\n",
    "    '''\n",
    "    # Read from csv file.\n",
    "    df = pd.read_csv(csv_name, index_col=1, parse_dates=True, names=[ 'server', 'value'], comment='#', quotechar=\"'\", sep=\",\")\n",
    "    if(df.isnull().value.any()): \n",
    "        #print (\"{0} - missing data row count#: {1}\".format(csv_name, np.count_nonzero(df1.isnull().values)))\n",
    "        df = df.fillna(method='ffill', limit=2) # Fill last value upto two consequtive slots\n",
    "    print(\"{0} --- #(rows:columns): {1}\".format(csv_name, df.shape))\n",
    "    return df\n",
    "\n",
    "def rename_series(series, prefix='v'):\n",
    "    s = series.unique()\n",
    "    newS = pd.Series(np.arange(len(s), dtype=np.int))\n",
    "    newS = newS.apply(lambda x:  prefix + str(x))\n",
    "    d = dict(zip(s, newS))\n",
    "    newSeries =series.map(d)\n",
    "    return newSeries\n",
    "\n",
    "def expand_df(df, ef=2, freq='D'):\n",
    "    d = df.copy()\n",
    "    d1 = pd.concat([df]*ef, ignore_index=True)\n",
    "    d1.index = pd.date_range(df.index[0], periods=len(d1), freq=freq)\n",
    "    return d1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##### MODEL API ###########\n",
    "\n",
    "# convert an array of values into a dataset matrix\n",
    "def create_dataset(dataset, look_back=1):\n",
    "    dataX, dataY = [], []\n",
    "    for i in range(len(dataset)-look_back-1):\n",
    "        a = dataset[i:(i+look_back), 0]\n",
    "        dataX.append(a)\n",
    "        dataY.append(dataset[i + look_back, 0])\n",
    "    return np.array(dataX), np.array(dataY)\n",
    "\n",
    "def model_rnn_lstm(dataframe, seed=9, title='title'):\n",
    "    # fix random seed for reproducibility\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    # load the dataset\n",
    "    df = dataframe\n",
    "    dataset = df.values\n",
    "    dataset = dataset.astype('float32')\n",
    "\n",
    "    # normalize the dataset\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    dataset = scaler.fit_transform(dataset)\n",
    "\n",
    "    # split into train and test sets\n",
    "    train_size = int(len(dataset) * 0.67)\n",
    "    test_size = len(dataset) - train_size\n",
    "    train, test = dataset[0:train_size,:], dataset[train_size:len(dataset),:]\n",
    "\n",
    "    # reshape into X=t and Y=t+1\n",
    "    look_back = 2\n",
    "    trainX, trainY = create_dataset(train, look_back)\n",
    "    testX, testY = create_dataset(test, look_back)\n",
    "\n",
    "    # reshape input to be [samples, time steps, features]\n",
    "    trainX = np.reshape(trainX, (trainX.shape[0], 1, trainX.shape[1]))\n",
    "    testX = np.reshape(testX, (testX.shape[0], 1, testX.shape[1]))\n",
    "    #trainX = np.reshape(trainX, (trainX.shape[0], trainX.shape[1], 1))\n",
    "    #testX = np.reshape(testX, (testX.shape[0], testX.shape[1], 1))\n",
    "\n",
    "    # create and fit the LSTM network\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(12, input_dim=look_back))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    model.fit(trainX, trainY, nb_epoch=200, batch_size=2, verbose=0)\n",
    "\n",
    "    # make predictions\n",
    "    trainPredict = model.predict(trainX)\n",
    "    testPredict = model.predict(testX)\n",
    "\n",
    "    # invert predictions\n",
    "    trainPredict = scaler.inverse_transform(trainPredict)\n",
    "    trainY = scaler.inverse_transform([trainY])\n",
    "    testPredict = scaler.inverse_transform(testPredict)\n",
    "    testY = scaler.inverse_transform([testY])\n",
    "\n",
    "    # calculate root mean squared error\n",
    "    trainScore = math.sqrt(mean_squared_error(trainY[0], trainPredict[:,0]))\n",
    "    testScore = math.sqrt(mean_squared_error(testY[0], testPredict[:,0]))\n",
    "    print('%s:, Train_Score: %.2f RMSE, Test_Score: %.2f RMSE' % (title, trainScore, testScore))\n",
    "\n",
    "    # shift train predictions for plotting\n",
    "    trainPredictPlot = np.empty_like(dataset)\n",
    "    trainPredictPlot[:, :] = np.nan\n",
    "    trainPredictPlot[look_back:len(trainPredict)+look_back, :] = trainPredict\n",
    "\n",
    "    # shift test predictions for plotting\n",
    "    testPredictPlot = np.empty_like(dataset)\n",
    "    testPredictPlot[:, :] = np.nan\n",
    "    testPredictPlot[len(trainPredict)+(look_back*2)+1:len(dataset)-1, :] = testPredict\n",
    "\n",
    "    # Plot base lines & predictions\n",
    "    fig = plt.figure(figsize=(12, 3))\n",
    "    ax = plt.gca()\n",
    "    plt.plot(scaler.inverse_transform(dataset), color='steelblue', linewidth=3, label='Original', alpha=0.7)\n",
    "    plt.plot(trainPredictPlot, color='sage', label='TrainPredict')\n",
    "    plt.plot(testPredictPlot, color='indianred', label='TestPredict')\n",
    "    plt.xlabel(\"x\", fontweight='bold')\n",
    "    plt.ylabel(\"GB\", fontweight='bold')\n",
    "    plt.title(title + \": \" + \"Baseline & Predictions\")\n",
    "    plt.legend(loc='best')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##### Montecarlo API ###########\n",
    "\n",
    "def get_days_forecast(df, freecapacity):\n",
    "    daysleft = 0\n",
    "    availcap = freecapacity\n",
    "    while availcap >= 0:\n",
    "        usage = np.random.choice(df, 1, replace=True)\n",
    "        availcap = availcap - usage\n",
    "        daysleft = daysleft + 1\n",
    "    return daysleft\n",
    "\n",
    "def montecarlo_days_forecast(df, freecapacity, iterations=10000):    \n",
    "    # Init\n",
    "    lastday = df.index[-1]\n",
    "    days_s = pd.Series(np.zeros(iterations, dtype=int), name='days_s')\n",
    "    dates_s = pd.Series(np.zeros(iterations), name='dates_s')\n",
    "    \n",
    "    #Forecast days & dates\n",
    "    for i in range(iterations):\n",
    "        days_s[i] = get_days_forecast(df, freecapacity)\n",
    "        dates_s[i] = lastday + timedelta(days_s[i])\n",
    "    days_s = days_s.dropna()\n",
    "    dates_s = dates_s.sort_values()\n",
    "    return (days_s, dates_s)\n",
    "\n",
    "\n",
    "def get_usage_forecast(df, days=100):\n",
    "    s = pd.Series(np.zeros(days, dtype=float), name='capacity_usage')\n",
    "    s[0] = 0\n",
    "    for i in range(1, days+1):\n",
    "        usage = np.random.choice(df, 1, replace=True)\n",
    "        s[i] = s[i-1]+usage\n",
    "    #pd.to_numeric(s, errors='coerce')\n",
    "    return s\n",
    "\n",
    "def montecarlo_usage_forecast(df, days=100, iterations=10000):\n",
    "    lastday = df.index[-1]\n",
    "    cap_index = pd.date_range(lastday, periods=days+1, freq='D')\n",
    "    cap_usage_df = pd.DataFrame(index=cap_index)\n",
    "    \n",
    "    for i in range(iterations):\n",
    "        series  = get_usage_forecast(df, days)\n",
    "        series.index = cap_index\n",
    "        cap_usage_df.loc[:, i] = series\n",
    "        \n",
    "    cap_usage_df = cap_usage_df.dropna()\n",
    "    return cap_usage_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##### PLOTTING API ###########\n",
    "\n",
    "\n",
    "def create_trend_plot(xlabel, ylabel, title, df):\n",
    "    # Create line chart\n",
    "    fig = plt.figure(figsize=(14, 4))\n",
    "    ax = plt.gca()\n",
    "    ax.xaxis.set_major_locator(DayLocator(interval=7))\n",
    "    ax.xaxis.set_major_formatter(DateFormatter('%Y-%m-%d')) # TimeFormat : http://strftime.org/\n",
    "    plt.plot(df.total, color='gray', linewidth=3, linestyle= '--', label='Total Capacity')\n",
    "    plt.plot(df.used, color='steelblue', label='Capacity Used')\n",
    "    plt.xlabel(xlabel, fontweight='bold')\n",
    "    plt.ylabel(ylabel, fontweight='bold')\n",
    "    plt.title(title)\n",
    "    plt.legend(loc='best')\n",
    "    plt.gcf().autofmt_xdate()\n",
    "    return\n",
    "\n",
    "def create_shape_plots(xlabel, ylabel, title, df):\n",
    "    #Create subplots\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(14, 4))\n",
    "    \n",
    "    # Box-Whiskers Plot\n",
    "    x = df.pivot(columns='week_days', values='diff')\n",
    "    x = x.rename(columns={0: 'Mon', 1: 'Tue', 2:'Wed', 3:'Thur', 4:'Fri', 5:'Sat', 6:'Sun'})\n",
    "    g1 = sns.violinplot(data=x, palette=sns.color_palette(), split=True, ax=axes[0])\n",
    "    g1.set_xlabel(xlabel, fontweight='bold', fontsize=12)\n",
    "    g1.set_ylabel(ylabel, fontweight='bold', fontsize=12)\n",
    "    g1.set_title(title)\n",
    "\n",
    "    # Swarm Plot\n",
    "    g2 = sns.stripplot(data=df['diff'], jitter=True, size=4, color=\".1\", linewidth=0, ax=axes[1])\n",
    "    g3 = sns.violinplot(data=df['diff'], palette=sns.color_palette(), split=True, ax=axes[1])\n",
    "    g3.set_xlabel('')\n",
    "    g3.set_ylabel(ylabel, fontweight='bold', fontsize=12)\n",
    "    g3.set_title(title)\n",
    "    return\n",
    "\n",
    "def create_ecdf_plots(ylabel, title, cap_full_days, cap_full_dates):\n",
    "    #Create subplots\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(14, 4))\n",
    "    \n",
    "    ecdf1 = sm.distributions.ECDF(cap_full_days)\n",
    "    x1 = np.linspace(min(cap_full_days), max(cap_full_days))\n",
    "    y1 = ecdf1(x1)\n",
    "    \n",
    "    ecdf2 = sm.distributions.ECDF(cap_full_dates)\n",
    "    x2 = cap_full_dates\n",
    "    y2 = ecdf2(x2)\n",
    "    \n",
    "    axes[0].step(x1, y1)\n",
    "    axes[0].set_title(title)\n",
    "    axes[0].set_xlabel(\"Days\", fontweight='bold')\n",
    "    axes[0].set_ylabel(ylabel, fontweight='bold')\n",
    "    \n",
    "    axes[1].step(x2, y2)\n",
    "    axes[1].set_title(title)\n",
    "    axes[1].set_xlabel(\"Date\", fontweight='bold')\n",
    "    axes[1].set_ylabel(ylabel, fontweight='bold')\n",
    "    plt.gcf().autofmt_xdate()\n",
    "    return\n",
    "\n",
    "def create_forecast_plot(xlabel, ylabel, title, df):\n",
    "    # Create line chart\n",
    "    fig = plt.figure(figsize=(14, 4))\n",
    "    ax = plt.gca()\n",
    "    #ax.xaxis.set_major_locator(DayLocator(interval=7))\n",
    "    #ax.xaxis.set_major_formatter(DateFormatter('%Y-%m-%d')) # TimeFormat : http://strftime.org/\n",
    "    plt.plot(df, color='slategray', label='Capacity Used', alpha=0.7)\n",
    "    plt.xlabel(xlabel, fontweight='bold')\n",
    "    plt.ylabel(ylabel, fontweight='bold')\n",
    "    plt.title(title)\n",
    "    plt.gcf().autofmt_xdate()\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Load & Cleanup..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/capacity/kelsey-capused-d.txt --- #(rows:columns): (4156, 2)\n",
      "data\\capacity\\dogfood-v1-capused-d.txt --- #(rows:columns): (92, 2)\n",
      "data\\capacity\\dogfood-v1-captotal-d.txt --- #(rows:columns): (92, 2)\n"
     ]
    }
   ],
   "source": [
    "# Read capacity used metric\n",
    "df_used1 = read_metric_data(\"data/capacity/kelsey-capused-d.txt\")\n",
    "df_used2 = read_metric_data(\"data\\capacity\\dogfood-v1-capused-d.txt\")\n",
    "df_total = read_metric_data(\"data\\capacity\\dogfood-v1-captotal-d.txt\")\n",
    "\n",
    "# revise server values\n",
    "df_used1.server = rename_series(df_used1.server, prefix=\"v\")\n",
    "df_used2.server = rename_series(df_used2.server, prefix=\"v\")\n",
    "df_total.server = rename_series(df_total.server, prefix=\"v\")\n",
    "\n",
    "# Preprocess Data\n",
    "df_used1.loc[:,('value')] = (df_used1.value/(1024*1024*1024)).round(2)\n",
    "df_used2.loc[:,('value')] = (df_used2.value/(1024*1024*1024)).round(2)\n",
    "df_total.loc[:,('value')] = (df_total.value/(1024*1024*1024)).round(2)\n",
    "\n",
    "# group data by server\n",
    "g1 = df_used1.groupby('server')\n",
    "g2 = df_used2.groupby('server')\n",
    "\n",
    "#df.loc[:,'diff'] = df.value.diff()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary statistics...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary stats across all servers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\durga\\Anaconda2\\lib\\site-packages\\numpy\\lib\\function_base.py:3834: RuntimeWarning: Invalid value encountered in percentile\n",
      "  RuntimeWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th>server</th>\n",
       "      <th colspan=\"8\" halign=\"left\">v0</th>\n",
       "      <th colspan=\"2\" halign=\"left\">v1</th>\n",
       "      <th>...</th>\n",
       "      <th colspan=\"2\" halign=\"left\">v97</th>\n",
       "      <th colspan=\"8\" halign=\"left\">v98</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>...</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>value</th>\n",
       "      <td>42.0</td>\n",
       "      <td>38.81</td>\n",
       "      <td>16.04</td>\n",
       "      <td>3.59</td>\n",
       "      <td>36.46</td>\n",
       "      <td>43.0</td>\n",
       "      <td>49.7</td>\n",
       "      <td>65.14</td>\n",
       "      <td>42.0</td>\n",
       "      <td>40.38</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.91</td>\n",
       "      <td>40.0</td>\n",
       "      <td>18.56</td>\n",
       "      <td>1.19</td>\n",
       "      <td>16.85</td>\n",
       "      <td>17.47</td>\n",
       "      <td>18.2</td>\n",
       "      <td>19.57</td>\n",
       "      <td>20.88</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 792 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "server    v0                                                  v1         \\\n",
       "       count   mean    std   min    25%   50%   75%    max count   mean   \n",
       "value   42.0  38.81  16.04  3.59  36.46  43.0  49.7  65.14  42.0  40.38   \n",
       "\n",
       "server  ...   v97         v98                                                 \n",
       "        ...   75%   max count   mean   std    min    25%   50%    75%    max  \n",
       "value   ...   NaN  7.91  40.0  18.56  1.19  16.85  17.47  18.2  19.57  20.88  \n",
       "\n",
       "[1 rows x 792 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Summary stats across all servers...\")\n",
    "summary = g1.describe().T\n",
    "#summary = summary.drop(\"count\", axis=0)\n",
    "summary = summary.round(2)\n",
    "summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelling (LSTM RNN) & Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type1: Cyclical Behavior\n"
     ]
    }
   ],
   "source": [
    "# expand data\n",
    "print(\"Type1: Cyclical Behavior\")\n",
    "vdf0 = expand_df(g1.get_group(\"v0\"), ef=5, freq='D')\n",
    "model_rnn_lstm(vdf0[['value']], seed=7, title=\"Volume-0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"Type2: Run Up & Flat Behavior\")\n",
    "vdf2 = expand_df(g2.get_group(\"v0\"), ef=2, freq='D')\n",
    "model_rnn_lstm(vdf2[['value']], seed=7, title=\"Volume-2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"Type3: Mixed (sideways & trend) Behavior\")\n",
    "vdf8 = expand_df(g1.get_group(\"v8\"), ef=2, freq='D')\n",
    "model_rnn_lstm(vdf8[['value']], seed=7, title=\"Volume-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Montecarlo Simulations & Analytics..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_used = df_used2.copy()\n",
    "df_total = df_total.copy()\n",
    "\n",
    "df_trend = pd.DataFrame(index=df_used.index)\n",
    "df_trend['total'] = df_total.value.round(2)\n",
    "df_trend['used'] = df_used.value.round(2)\n",
    "\n",
    "df_diff = pd.DataFrame(index= df_used.index)\n",
    "df_diff['diff'] = df_used.value.diff()\n",
    "df_diff = df_diff.dropna()\n",
    "df_diff['week_days'] = df_diff.index.weekday"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "days = 100\n",
    "simulations = 1000\n",
    "\n",
    "totalcap = df_total.value.iloc[1]\n",
    "usedcap = df_used.value.iloc[-1]\n",
    "freecapacity = totalcap - usedcap\n",
    "\n",
    "cap_full_days, cap_full_dates = montecarlo_days_forecast(df_diff['diff'], freecapacity, simulations)\n",
    "cap_usage_df = montecarlo_usage_forecast(df_diff['diff'], days, simulations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "create_trend_plot(\"Date\", \"Capacity (GB)\", \"Volume-2: Capacity Usage Trend\", df_trend)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"* Notice the width and height of bars in patterns. width = how often. height = how far.\")\n",
    "create_shape_plots(\"\",\"Capacity (GB)\", \"Capacity Usage Patterns (Daily)\", df_diff)\n",
    "create_forecast_plot(\"Date\", \"Capacity (GB)\", \"Capacity Usage Forecast\", cap_usage_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "create_ecdf_plots(\"Probability\",\"Days to full capacity...\", cap_full_days, cap_full_dates)\n",
    "print(\"* Generally 0.8 probability is a good target for full capacity...\")"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
