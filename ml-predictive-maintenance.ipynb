{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### USX Predictive Maintenance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The business problem for this notebook is about predicting problems caused by component failures such that the question \"What is the probability that a machine will fail in the near future due to a failure of a certain component?\" can be answered. The problem is formatted as a multi-class classification problem and a machine learning algorithm is used to create the predictive model that learns from historical data collected from machines. \n",
    "\n",
    "The following sections go through the steps of implementing such a model which are feature engineering, label construction, training and evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# CSS Files\n",
    "from IPython.core.display import HTML\n",
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# general libs\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import itertools\n",
    "\n",
    "#plotting libs\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.dates import DayLocator, HourLocator, MinuteLocator, AutoDateLocator, DateFormatter\n",
    "\n",
    "# date time libs\n",
    "import datetime as dt\n",
    "from datetime import timedelta\n",
    "import statsmodels.api as sm \n",
    "\n",
    "# ml libs\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from pandas.tools.plotting import scatter_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# CSS Fileb\n",
    "from IPython.core.display import HTML\n",
    "from IPython.display import Image\n",
    "\n",
    "# Style Setting\n",
    "sns.set(style=\"ticks\", color_codes=True)\n",
    "sns.set_context(\"notebook\")\n",
    "sns.set_style(\"darkgrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Notebook Style Setting\n",
    "css = open('styles/style-table.css').read() + open('styles/style-notebook.css').read()\n",
    "HTML('<style>{}</style>'.format(css))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "HTML('''<script>\n",
    "code_show=true; \n",
    "function code_toggle() {\n",
    " if (code_show){\n",
    " $('div.input').hide();\n",
    " } else {\n",
    " $('div.input').show();\n",
    " }\n",
    " code_show = !code_show\n",
    "} \n",
    "$( document ).ready(code_toggle);\n",
    "</script>\n",
    "<form action=\"javascript:code_toggle()\"><input type=\"submit\" value=\"Toggle Code On/Off\"></form>''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Sources\n",
    "\n",
    "Common data sources for predictive maintenance problems are :\n",
    " 1. Failure history: The failure history of a node or component within the node. Ex: Alerts\n",
    " 2. Machine usage. The operating conditions like metric data collected from sensors. Ex: Telemetry\n",
    " 3. Machine features: The static info of a node. Ex: Config info like num cpus, memory, network etc.\n",
    " 4. Error Events: The errors in logs but not has tranlated into actual failures. \n",
    " 5. Admin Actions: The behaviorial info like API accesses, Jobs run etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading Data..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "telemetry = pd.read_csv('data/pdm/pdm_telemetry2.csv', index_col=0, parse_dates=True)\n",
    "errors = pd.read_csv('data/pdm/pdm_errors.csv', index_col=0, parse_dates=True)\n",
    "ops = pd.read_csv('data/pdm/pdm_ops.csv', index_col=0, parse_dates=True)\n",
    "alerts = pd.read_csv('data/pdm/pdm_alerts.csv', index_col=0, parse_dates=True)\n",
    "machines = pd.read_csv('data/pdm/pdm_machines.csv', index_col=0, parse_dates=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleanup..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Processing telemetry data..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first data source is the telemetry time-series data which consists of metrics m1, m2, m3, m4 etc collected from 100 machines in real time averaged over every hour collected. Below, we display the first 10 records in the dataset. A summary of the whole dataset is also provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "telemetry = telemetry.dropna()\n",
    "telemetry['datetime'] = telemetry.index\n",
    "telemetry.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# plot telemetry data\n",
    "plot_df = telemetry.loc[(telemetry['machineID'] == 1) &\n",
    "                        (telemetry['datetime'] > pd.to_datetime('2015-01-01')) &\n",
    "                        (telemetry['datetime'] < pd.to_datetime('2015-02-01')), ['datetime', 'm1']]\n",
    "\n",
    "sns.set_style(\"darkgrid\")\n",
    "plt.figure(figsize=(12, 3))\n",
    "plt.plot(plot_df['datetime'], plot_df['m1'])\n",
    "plt.ylabel('metric1')\n",
    "\n",
    "# make x-axis ticks legible\n",
    "adf = plt.gca().get_xaxis().get_major_formatter()\n",
    "adf.scaled[1.0] = '%m-%d'\n",
    "plt.xlabel('Date')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Processing error data...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second major data source is the error logs. These are non-breaking errors thrown while the machine is still operational and do not constitute as failures. The error date and times are rounded to the closest hour since the telemetry data is collected at an hourly rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "errors = errors.dropna()\n",
    "errors['datetime'] = errors.index\n",
    "errors['errorID'] = errors['errorID'].astype('category')\n",
    "\n",
    "print(\"Total number of error records: %d\" % len(errors.index))\n",
    "errors.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot error data\n",
    "sns.set_style(\"darkgrid\")\n",
    "plt.figure(figsize=(6, 3))\n",
    "errors['errorID'].value_counts().plot(kind='bar')\n",
    "plt.ylabel('Count')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Processing jobs/ops data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the scheduled and unscheduled admin operations corresponding to both regular inspection of components as well as failures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ops = ops.dropna()\n",
    "ops['datetime'] = ops.index\n",
    "ops['optype'] = ops['optype'].astype('category')\n",
    "\n",
    "print(\"Total number of operation records: %d\" % len(ops.index))\n",
    "ops.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sns.set_style(\"darkgrid\")\n",
    "plt.figure(figsize=(6, 3))\n",
    "ops['optype'].value_counts().plot(kind='bar')\n",
    "plt.ylabel('Count')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Processing machines (config) data...\n",
    "\n",
    "This data set includes some config information about the machines: metrics: cm1, cm2, ...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "machines = machines.dropna()\n",
    "machines['machineID'] = machines.index\n",
    "machines['model'] = machines['model'].astype('category')\n",
    "machines.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sns.set_style(\"darkgrid\")\n",
    "plt.figure(figsize=(6, 4))\n",
    "_, bins, _ = plt.hist([machines.loc[machines['model'] == 'model1', 'cm1'],\n",
    "                       machines.loc[machines['model'] == 'model2', 'cm1'],\n",
    "                       machines.loc[machines['model'] == 'model3', 'cm1'],\n",
    "                       machines.loc[machines['model'] == 'model4', 'cm1']],\n",
    "                       20, stacked=True, label=['model1', 'model2', 'model3', 'model4'])\n",
    "plt.xlabel('config metric 1 (cm1)')\n",
    "plt.ylabel('Count')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Processing alert data...\n",
    "\n",
    "These are the records of due to failures. Each record has a date and time, machine ID, and alert type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "alerts = alerts.dropna()\n",
    "alerts['datetime'] = alerts.index\n",
    "alerts['alerttype'] = failures['alerttype'].astype('category')\n",
    "\n",
    "print(\"Total number of alerts: %d\" % len(alerts.index))\n",
    "alerts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sns.set_style(\"darkgrid\")\n",
    "plt.figure(figsize=(6, 4))\n",
    "alerts['alerttype'].value_counts().plot(kind='bar')\n",
    "plt.ylabel('Count')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step in predictive maintenance is feature engineering which requires bringing the different data sources together to create features that best describe a machines's health condition at a given point in time. In the next sections, several feature engineering methods are used to create features based on the properties of each data source.\n",
    "\n",
    "#### Lag features in telemetry data...\n",
    "\n",
    "Telemetry data almost always comes with time-stamps which makes it suitable for calculating lagging features. A common method is to pick a window size for the lag features to be created and compute rolling aggregate measures such as mean, standard deviation, minimum, maximum, etc. to represent the short term history of the telemetry over the lag window. In the following, rolling mean and standard deviation of the telemetry data over the last 3 hour lag window is calculated for every 3 hours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Capturing 3 hour lag...\n",
    "\n",
    "# Calculate mean values for telemetry features\n",
    "temp = []\n",
    "fields = ['m1', 'm2', 'm3', 'm4']\n",
    "for col in fields:\n",
    "    temp.append(pd.pivot_table(telemetry,\n",
    "                               index='datetime',\n",
    "                               columns='machineID',\n",
    "                               values=col).resample('3H', closed='left', label='right').mean().unstack())\n",
    "telemetry_mean_3h = pd.concat(temp, axis=1)\n",
    "telemetry_mean_3h.columns = [i + 'mean_3h' for i in fields]\n",
    "telemetry_mean_3h.reset_index(inplace=True)\n",
    "\n",
    "# repeat for standard deviation\n",
    "temp = []\n",
    "for col in fields:\n",
    "    temp.append(pd.pivot_table(telemetry,\n",
    "                               index='datetime',\n",
    "                               columns='machineID',\n",
    "                               values=col).resample('3H', closed='left', label='right').std().unstack())\n",
    "telemetry_sd_3h = pd.concat(temp, axis=1)\n",
    "telemetry_sd_3h.columns = [i + 'sd_3h' for i in fields]\n",
    "telemetry_sd_3h.reset_index(inplace=True)\n",
    "\n",
    "telemetry_mean_3h.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For capturing a longer term effect, 24 hour lag features are also calculated as below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Capturing longer term i.e., 24 hour lag...\n",
    "temp = []\n",
    "fields = ['m1', 'm2', 'm3', 'm4']\n",
    "for col in fields:\n",
    "    temp.append(pd.rolling_mean(pd.pivot_table(telemetry,\n",
    "                                               index='datetime',\n",
    "                                               columns='machineID',\n",
    "                                               values=col), window=24).resample('3H',\n",
    "                                                                                closed='left',\n",
    "                                                                                label='right').first().unstack())\n",
    "telemetry_mean_24h = pd.concat(temp, axis=1)\n",
    "telemetry_mean_24h.columns = [i + 'mean_24h' for i in fields]\n",
    "telemetry_mean_24h.reset_index(inplace=True)\n",
    "telemetry_mean_24h = telemetry_mean_24h.loc[-telemetry_mean_24h['m1mean_24h'].isnull()]\n",
    "\n",
    "# repeat for standard deviation\n",
    "temp = []\n",
    "fields = ['m1', 'm2', 'm3', 'm4']\n",
    "for col in fields:\n",
    "    temp.append(pd.rolling_std(pd.pivot_table(telemetry,\n",
    "                                               index='datetime',\n",
    "                                               columns='machineID',\n",
    "                                               values=col), window=24).resample('3H',\n",
    "                                                                                closed='left',\n",
    "                                                                                label='right').first().unstack())\n",
    "telemetry_sd_24h = pd.concat(temp, axis=1)\n",
    "telemetry_sd_24h.columns = [i + 'sd_24h' for i in fields]\n",
    "telemetry_sd_24h.reset_index(inplace=True)\n",
    "telemetry_sd_24h = telemetry_sd_24h.loc[-telemetry_sd_24h['m1sd_24h'].isnull()]\n",
    "\n",
    "# Notice that a 24h rolling average is not available at the earliest timepoints\n",
    "telemetry_mean_24h.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, the columns of the feature datasets created earlier are merged to create the final feature set from telemetry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# merge columns of feature sets created earlier\n",
    "telemetry_feat = pd.concat([telemetry_mean_3h,\n",
    "                            telemetry_sd_3h.ix[:, 2:6],\n",
    "                            telemetry_mean_24h.ix[:, 2:6],\n",
    "                            telemetry_sd_24h.ix[:, 2:6]], axis=1).dropna()\n",
    "telemetry_feat.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "telemetry_feat.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lag features in error data...\n",
    "\n",
    "Like telemetry data, errors come with timestamps. An important difference is that the error IDs are categorical values and should not be averaged over time intervals like the telemetry measurements. Instead, we count the number of errors of each type in a lagging window. We begin by reformatting the error data to have one entry per machine per time at which at least one error occurred:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "errors.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create a column for each error type\n",
    "error_count = pd.get_dummies(errors.set_index('datetime')).reset_index()\n",
    "error_count.columns = ['datetime', 'machineID', 'error1', 'error2', 'error3', 'error4', 'error5']\n",
    "\n",
    "# combine errors for a given machine in a given hour\n",
    "error_count_g = error_count.groupby(['machineID', 'datetime']).sum().reset_index()\n",
    "error_count_g.head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we add blank entries for all other hourly timepoints (since no errors occurred at those times):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "error_count = error_count_g.copy()\n",
    "error_count = telemetry[['datetime', 'machineID']].merge(error_count, on=['machineID', 'datetime'], how='left').fillna(0.0)\n",
    "error_count.describe()\n",
    "error_count.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Now we compute the total number of errors of each type over the last 24 hours, for timepoints taken every three hours:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "temp = []\n",
    "fields = ['error%d' % i for i in range(1,6)]\n",
    "print(fields)\n",
    "for col in fields:\n",
    "    temp.append(pd.rolling_sum(\n",
    "            pd.pivot_table(error_count, index='datetime', columns='machineID', values=col), \n",
    "            window=24).resample('3H', closed='left', label='right', how='first').unstack())\n",
    "    \n",
    "error_count = pd.concat(temp, axis=1)\n",
    "error_count.columns = [i + 'count' for i in fields]\n",
    "error_count.reset_index(inplace=True)\n",
    "error_count = error_count.dropna()\n",
    "error_count.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "error_count.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lag features in ops data...\n",
    "Creating lagging features from ops data is not as straightforward as for telemetry and errors, so the features from this data are generated in a more custom way. This type of ad-hoc feature engineering is very common in predictive maintenance since domain knowledge plays a big role in understanding the predictors of a problem. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# create a column for each op type\n",
    "ops_rep = pd.get_dummies(ops.set_index('datetime')).reset_index()\n",
    "ops_rep.columns = ['datetime', 'machineID', 'op1', 'op2', 'op3', 'op4']\n",
    "\n",
    "# combine operation for a given machine in a given hour\n",
    "ops_rep = ops_rep.groupby(['machineID', 'datetime']).sum().reset_index()\n",
    "\n",
    "# add timepoints where no operations were done\n",
    "ops_rep = telemetry[['datetime', 'machineID']].merge(ops_rep,\n",
    "                                                      on=['datetime', 'machineID'],\n",
    "                                                      how='outer').fillna(0).sort_values(by=['machineID', 'datetime'])\n",
    "\n",
    "ops = ['op1', 'op2', 'op3', 'op4']\n",
    "for op in ops:\n",
    "    # convert indicator to most recent date of op \n",
    "    ops_rep.loc[ops_rep[op] < 1, op] = None\n",
    "    ops_rep.loc[-ops_rep[op].isnull(), op] = ops_rep.loc[-ops_rep[op].isnull(), 'datetime']\n",
    "    \n",
    "    # forward-fill the most-recent date of op \n",
    "    ops_rep[op] = ops_rep[op].fillna(method='ffill')\n",
    "\n",
    "# remove dates in 2014 (may have NaN or future component change dates)    \n",
    "ops_rep = ops_rep.loc[ops_rep['datetime'] > pd.to_datetime('2015-01-01')]\n",
    "\n",
    "# replace dates of most recent ops  with days since most recent op change\n",
    "for op in components:\n",
    "    ops_rep[op] = (ops_rep['datetime'] - ops_rep[op]) / np.timedelta64(1, 'D')\n",
    "    \n",
    "ops_rep.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ops_rep.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Machine features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The machine features can be used without further modification. These include descriptive information about the type of each machine and its config metrics.\n",
    "\n",
    "#### Final merge....\n",
    "Finally, we merge all the feature data sets we created earlier to get the final feature matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "final_feat = telemetry_feat.merge(error_count, on=['datetime', 'machineID'], how='left')\n",
    "final_feat = final_feat.merge(ops_rep, on=['datetime', 'machineID'], how='left')\n",
    "final_feat = final_feat.merge(machines, on=['machineID'], how='left')\n",
    "final_feat.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "final_feat.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Label Construction\n",
    "\n",
    "When using multi-class classification for predicting failure due to a problem, labelling is done by taking a time window prior to the failure of an asset and labelling the feature records that fall into that window as \"about to fail due to a problem\" while labelling all other records as \"normal.\" This time window should be picked according to the business case: in some situations it may be enough to predict failures hours in advance, while in others days or weeks may be needed to allow e.g. for arrival of replacement parts.\n",
    "\n",
    "The prediction problem for this example scenerio is to estimate the probability that a machine will fail in the near future due to a failure of a certain component. More specifically, the goal is to compute the probability that a machine will fail in the next 24 hours due to a certain component failure (component 1, 2, 3, or 4). Below, a categorical failure feature is created to serve as the label. All records within a 24 hour window before a failure of component 1 have failure=comp1, and so on for components 2, 3, and 4; all records not within 24 hours of a component failure have failure=none.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "labeled_features = final_feat.merge(alerts, on=['datetime', 'machineID'], how='left')\n",
    "labeled_features = labeled_features.fillna(method='bfill', limit=7) # fill backward up to 24h\n",
    "labeled_features = labeled_features.fillna('none')\n",
    "labeled_features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is an example of records that are labeled as failure=comp4 in the failure column. Notice that the first 8 records all occur in the 24-hour window before the first recorded failure of component 4. The next 8 records are within the 24 hour window before another failure of component 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "labeled_features.loc[labeled_features['alerttype'] == 'alert4'][:16]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Training, Validation and Testing\n",
    "\n",
    "When working with time-stamped data as in this example, record partitioning into training, validation, and test sets should be performed carefully to prevent overestimating the performance of the models. \n",
    "\n",
    "In predictive maintenance, the features are usually generated using lagging aggregates: records in the same time window will likely have identical labels and similar feature values. These correlations can give a model an \"unfair advantage\" when predicting on a test set record that shares its time window with a training set record. We therefore partition records into training, validation, and test sets in large chunks, to minimize the number of time intervals shared between them.\n",
    "\n",
    "Predictive models have no advance knowledge of future chronological trends: in practice, such trends are likely to exist and to adversely impact the model's performance. To obtain an accurate assessment of a predictive model's performance, we recommend training on older records and validating/testing using newer records.\n",
    "\n",
    "For both of these reasons, a time-dependent record splitting strategy is an excellent choice for predictive maintenace models. The split is effected by choosing a point in time based on the desired size of the training and test sets: all records before the timepoint are used for training the model, and all remaining records are used for testing. (If desired, the timeline could be further divided to create validation sets for parameter selection.) To prevent any records in the training set from sharing time windows with the records in the test set, we remove any records at the boundary -- in this case, by ignoring 24 hours' worth of data prior to the timepoint.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# make test and training splits\n",
    "threshold_dates = [[pd.to_datetime('2015-07-31 01:00:00'), pd.to_datetime('2015-08-01 01:00:00')],\n",
    "                   [pd.to_datetime('2015-08-31 01:00:00'), pd.to_datetime('2015-09-01 01:00:00')],\n",
    "                   [pd.to_datetime('2015-09-30 01:00:00'), pd.to_datetime('2015-10-01 01:00:00')]]\n",
    "\n",
    "test_results = []\n",
    "models = []\n",
    "for last_train_date, first_test_date in threshold_dates:\n",
    "    # split out training and test data\n",
    "    train_y = labeled_features.loc[labeled_features['datetime'] < last_train_date, 'alerttype']\n",
    "    train_X = pd.get_dummies(labeled_features.loc[labeled_features['datetime'] < last_train_date].drop(['datetime',\n",
    "                                                                                                        'machineID',\n",
    "                                                                                                        'alerttype'], 1))\n",
    "    test_X = pd.get_dummies(labeled_features.loc[labeled_features['datetime'] > first_test_date].drop(['datetime',\n",
    "                                                                                                       'machineID',\n",
    "                                                                                                       'alerttype'], 1))\n",
    "    # train and predict using the model, storing results for later\n",
    "    my_model = GradientBoostingClassifier(random_state=42)\n",
    "    my_model.fit(train_X, train_y)\n",
    "    test_result = pd.DataFrame(labeled_features.loc[labeled_features['datetime'] > first_test_date])\n",
    "    test_result['predicted_failure'] = my_model.predict(test_X)\n",
    "    test_results.append(test_result)\n",
    "    models.append(my_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we plot the feature importances in the (first) trained model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sns.set_style(\"darkgrid\")\n",
    "plt.figure(figsize=(12, 4))\n",
    "labels, importances = zip(*sorted(zip(test_X.columns, models[0].feature_importances_), reverse=True, key=lambda x: x[1]))\n",
    "plt.xticks(range(len(labels)), labels)\n",
    "_, labels = plt.xticks()\n",
    "plt.setp(labels, rotation=90)\n",
    "plt.bar(range(len(importances)), importances)\n",
    "plt.ylabel('Importance')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "\n",
    "In predictive maintenance, machine failures are usually rare occurrences in the lifetime of the assets compared to normal operation. This causes an imbalance in the label distribution which usually causes poor performance as algorithms tend to classify majority class examples better at the expense of minority class examples as the total misclassification error is much improved when majority class is labeled correctly. \n",
    "\n",
    "This causes low recall rates although accuracy can be high and becomes a larger problem when the cost of false alarms to the business is very high. To help with this problem, sampling techniques such as oversampling of the minority examples are usually used along with more sophisticated techniques which are not covered in this notebook.\n",
    "\n",
    "Link: https://github.com/scikit-learn-contrib/imbalanced-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sns.set_style(\"darkgrid\")\n",
    "plt.figure(figsize=(8, 4))\n",
    "labeled_features['alerttype'].value_counts().plot(kind='bar')\n",
    "plt.xlabel('Alert Occurence')\n",
    "plt.ylabel('Count')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, due to the class imbalance problem, it is important to look at evaluation metrics other than accuracy alone and compare those metrics to the baseline metrics which are computed when random chance is used to make predictions rather than a machine learning model. The comparison will bring out the value and benefits of using a machine learning model better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, recall_score, accuracy_score, precision_score\n",
    "\n",
    "def Evaluate(predicted, actual, labels):\n",
    "    output_labels = []\n",
    "    output = []\n",
    "    \n",
    "    # Calculate and display confusion matrix\n",
    "    cm = confusion_matrix(actual, predicted, labels=labels)\n",
    "    print('Confusion matrix\\n- x-axis is true labels (none, comp1, etc.)\\n- y-axis is predicted labels')\n",
    "    print(cm)\n",
    "    \n",
    "    # Calculate precision, recall, and F1 score\n",
    "    accuracy = np.array([float(np.trace(cm)) / np.sum(cm)] * len(labels))\n",
    "    precision = precision_score(actual, predicted, average=None, labels=labels)\n",
    "    recall = recall_score(actual, predicted, average=None, labels=labels)\n",
    "    f1 = 2 * precision * recall / (precision + recall)\n",
    "    output.extend([accuracy.tolist(), precision.tolist(), recall.tolist(), f1.tolist()])\n",
    "    output_labels.extend(['accuracy', 'precision', 'recall', 'F1'])\n",
    "    \n",
    "    # Calculate the macro versions of these metrics\n",
    "    output.extend([[np.mean(precision)] * len(labels),\n",
    "                   [np.mean(recall)] * len(labels),\n",
    "                   [np.mean(f1)] * len(labels)])\n",
    "    output_labels.extend(['macro precision', 'macro recall', 'macro F1'])\n",
    "    \n",
    "    # Find the one-vs.-all confusion matrix\n",
    "    cm_row_sums = cm.sum(axis = 1)\n",
    "    cm_col_sums = cm.sum(axis = 0)\n",
    "    s = np.zeros((2, 2))\n",
    "    for i in range(len(labels)):\n",
    "        v = np.array([[cm[i, i],\n",
    "                       cm_row_sums[i] - cm[i, i]],\n",
    "                      [cm_col_sums[i] - cm[i, i],\n",
    "                       np.sum(cm) + cm[i, i] - (cm_row_sums[i] + cm_col_sums[i])]])\n",
    "        s += v\n",
    "    s_row_sums = s.sum(axis = 1)\n",
    "    \n",
    "    # Add average accuracy and micro-averaged  precision/recall/F1\n",
    "    avg_accuracy = [np.trace(s) / np.sum(s)] * len(labels)\n",
    "    micro_prf = [float(s[0,0]) / s_row_sums[0]] * len(labels)\n",
    "    output.extend([avg_accuracy, micro_prf])\n",
    "    output_labels.extend(['average accuracy',\n",
    "                          'micro-averaged precision/recall/F1'])\n",
    "    \n",
    "    # Compute metrics for the majority classifier\n",
    "    mc_index = np.where(cm_row_sums == np.max(cm_row_sums))[0][0]\n",
    "    cm_row_dist = cm_row_sums / float(np.sum(cm))\n",
    "    mc_accuracy = 0 * cm_row_dist; mc_accuracy[mc_index] = cm_row_dist[mc_index]\n",
    "    mc_recall = 0 * cm_row_dist; mc_recall[mc_index] = 1\n",
    "    mc_precision = 0 * cm_row_dist\n",
    "    mc_precision[mc_index] = cm_row_dist[mc_index]\n",
    "    mc_F1 = 0 * cm_row_dist;\n",
    "    mc_F1[mc_index] = 2 * mc_precision[mc_index] / (mc_precision[mc_index] + 1)\n",
    "    output.extend([mc_accuracy.tolist(), mc_recall.tolist(),\n",
    "                   mc_precision.tolist(), mc_F1.tolist()])\n",
    "    output_labels.extend(['majority class accuracy', 'majority class recall',\n",
    "                          'majority class precision', 'majority class F1'])\n",
    "        \n",
    "    # Random accuracy and kappa\n",
    "    cm_col_dist = cm_col_sums / float(np.sum(cm))\n",
    "    exp_accuracy = np.array([np.sum(cm_row_dist * cm_col_dist)] * len(labels))\n",
    "    kappa = (accuracy - exp_accuracy) / (1 - exp_accuracy)\n",
    "    output.extend([exp_accuracy.tolist(), kappa.tolist()])\n",
    "    output_labels.extend(['expected accuracy', 'kappa'])\n",
    "    \n",
    "\n",
    "    # Random guess\n",
    "    rg_accuracy = np.ones(len(labels)) / float(len(labels))\n",
    "    rg_precision = cm_row_dist\n",
    "    rg_recall = np.ones(len(labels)) / float(len(labels))\n",
    "    rg_F1 = 2 * cm_row_dist / (len(labels) * cm_row_dist + 1)\n",
    "    output.extend([rg_accuracy.tolist(), rg_precision.tolist(),\n",
    "                   rg_recall.tolist(), rg_F1.tolist()])\n",
    "    output_labels.extend(['random guess accuracy', 'random guess precision',\n",
    "                          'random guess recall', 'random guess F1'])\n",
    "    \n",
    "    # Random weighted guess\n",
    "    rwg_accuracy = np.ones(len(labels)) * sum(cm_row_dist**2)\n",
    "    rwg_precision = cm_row_dist\n",
    "    rwg_recall = cm_row_dist\n",
    "    rwg_F1 = cm_row_dist\n",
    "    output.extend([rwg_accuracy.tolist(), rwg_precision.tolist(),\n",
    "                   rwg_recall.tolist(), rwg_F1.tolist()])\n",
    "    output_labels.extend(['random weighted guess accuracy',\n",
    "                          'random weighted guess precision',\n",
    "                          'random weighted guess recall',\n",
    "                          'random weighted guess F1'])\n",
    "\n",
    "    output_df = pd.DataFrame(output, columns=labels)\n",
    "    output_df.index = output_labels\n",
    "                  \n",
    "    return output_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "evaluation_results = []\n",
    "for i, test_result in enumerate(test_results):\n",
    "    print('\\nSplit %d:' % (i+1))\n",
    "    evaluation_result = Evaluate(actual = test_result['alerttype'],\n",
    "                                 predicted = test_result['predicted_failure'],\n",
    "                                 labels = ['none', 'alert1', 'alert2', 'alert3', 'alert4'])\n",
    "    evaluation_results.append(evaluation_result)\n",
    "    \n",
    "evaluation_results[0]  # show full results for first split only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In predictive maintenance, we are often most concerned with how many of the actual failures were predicted by the model, i.e. the model's recall. (Recall becomes more important as the consequences of false negatives -- true failures that the model did not predict -- exceed the consequences of false positives, viz. false prediction of impending failure.) \n",
    "\n",
    "Below, we compare the recall rates for each failure type for the three models. The recall rates for all components as well as no failure are all above 90% meaning the model was able to capture above 90% of the failures correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "recall_df = pd.DataFrame([evaluation_results[0].loc['recall'].values,\n",
    "                          evaluation_results[1].loc['recall'].values,\n",
    "                          evaluation_results[2].loc['recall'].values],\n",
    "                         columns = ['none', 'alert1', 'alert2', 'alert3', 'alert4'],\n",
    "                         index = ['recall for first split',\n",
    "                                  'recall for second split',\n",
    "                                  'recall for third split'])\n",
    "recall_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, the steps of implementing a predictive maintenance model is provided using an example scenario where the goal is to predict alerts on a machine. Typical steps of predictive maintenance such as feature engineering, labelling, training and evaluation are done using a synthetic data set."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
